"""
Platform Policy Explorer Dashboard
Tier 1 Social Media Platforms
"""
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import json
from pathlib import Path
import pandas as pd

# Page config
st.set_page_config(
    page_title="Platform Policy Explorer",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Load data
@st.cache_data
def load_data():
    # Get absolute path to data directory
    # dashboard/app.py -> ASML3/dashboard, so data is at ASML3/dashboard/data
    current_file = Path(__file__).resolve()
    dashboard_dir = current_file.parent  # ASML3/dashboard
    data_dir = dashboard_dir / 'data'    # ASML3/dashboard/data
    
    viz_path = data_dir / 'tier1_visualization_data.json'
    scores_path = data_dir / 'tier1_dimension_scores.json'
    hybrid_path = data_dir / 'tier1_hybrid_temporal.json'
    discretion_path = data_dir / 'tier1_discretion_scores.json'
    pca_path = data_dir / 'tier1_pca_index.json'
    its_path = data_dir / 'tier1_its_analysis.json'
    
    with open(viz_path) as f:
        viz_data = json.load(f)
    with open(scores_path) as f:
        platform_scores = json.load(f)
    with open(hybrid_path) as f:
        hybrid_temporal = json.load(f)
    
    # Load enhanced methodology data if available
    try:
        with open(discretion_path) as f:
            discretion_scores = json.load(f)
    except FileNotFoundError:
        discretion_scores = {}
        
    try:
        with open(pca_path) as f:
            pca_index = json.load(f)
    except FileNotFoundError:
        pca_index = {}
        
    try:
        with open(its_path) as f:
            its_analysis = json.load(f)
    except FileNotFoundError:
        its_analysis = {}
        
    return viz_data, platform_scores, hybrid_temporal, discretion_scores, pca_index, its_analysis

viz_data, platform_scores, hybrid_temporal, discretion_scores, pca_index, its_analysis = load_data()

# Platform colors and metadata (10 platforms with real text analysis)
PLATFORM_CONFIG = {
    "Meta": {"color": "#1877F2", "confidence": "high", "emoji": "", "category": "social_media"},
    "YouTube": {"color": "#FF0000", "confidence": "high", "emoji": "", "category": "social_media"},
    "WhatsApp": {"color": "#25D366", "confidence": "medium", "emoji": "", "category": "messaging"},
    "Reddit": {"color": "#FF4500", "confidence": "high", "emoji": "", "category": "social_media"},
    "LinkedIn": {"color": "#0A66C2", "confidence": "high", "emoji": "", "category": "social_media"},
    "TikTok": {"color": "#000000", "confidence": "low", "emoji": "‚ö†Ô∏è", "category": "social_media"},
    "Twitter": {"color": "#1DA1F2", "confidence": "medium", "emoji": "", "category": "social_media"},
    "Instagram": {"color": "#E4405F", "confidence": "high", "emoji": "", "category": "social_media"},
    "Pinterest": {"color": "#BD081C", "confidence": "high", "emoji": "", "category": "social_media"},
    "Discord": {"color": "#5865F2", "confidence": "high", "emoji": "", "category": "social_media"},
}

# Header
st.title("üìä Platform Policy Explorer")

st.markdown("---")

# Sidebar filters
st.sidebar.header("‚öôÔ∏è Filters")
selected_platforms = st.sidebar.multiselect(
    "Select Platforms",
    options=list(platform_scores.keys()),
    default=list(platform_scores.keys())
)

show_confidence = st.sidebar.checkbox("Show Confidence Indicators", value=True)

# Main content tabs
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9, tab10 = st.tabs([
    "üìä Rankings",
    "üî∫ Dimensions",
    "‚öñÔ∏è Agency Asymmetry",
    "üìú Discretion",
    "üìã Detailed Metrics",
    "üìà Time Series",
    "üîç Regulatory Evasion",
    "üìÑ Privacy vs ToS",
    "üî¨ Causal Analysis",
    "‚úÖ Validation"
])

# TAB 1: RANKINGS
with tab1:
    st.subheader("Platform Rankings by Policy Index")
    
    if pca_index:
        # NEW: Show methodology badge
        st.info(f"""
        üìä **PCA-Weighted Index** (PC1 explains {pca_index['methodology']['explained_variance']:.1%} of variance)
        
        Dimensions: {', '.join(pca_index['methodology']['dimensions'])}
        """)
        
        # Build ranking from PCA results
        pca_ranking = []
        for platform, data in pca_index['platforms'].items():
            if platform in selected_platforms:
                pca_ranking.append({
                    'platform': platform,
                    'index': data['pca_index'],
                    'rank': data['rank']
                })
        
        pca_ranking.sort(key=lambda x: x['index'], reverse=True)
        
        # Create bar chart
        fig = go.Figure()
        
        for item in pca_ranking:
            platform = item['platform']
            config = PLATFORM_CONFIG[platform]
            
            label = f"{platform} {config['emoji']}" if show_confidence and config['emoji'] else platform
            
            fig.add_trace(go.Bar(
                x=[item['index']],
                y=[label],
                orientation='h',
                marker_color=config['color'],
                name=platform,
                text=[f"{item['index']:+.2f}"],
                textposition='outside',
                showlegend=False
            ))
        
        fig.update_layout(
            xaxis_title="PCA Policy Index (higher = more platform-protective)",
            yaxis_title="",
            height=400,
            yaxis={'categoryorder': 'total ascending'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # NEW: Show PCA loadings
        with st.expander("üìê How dimensions contribute to the index"):
            loadings = pca_index['methodology']['loadings']
            
            loading_data = []
            for dim, loading in loadings.items():
                loading_data.append({
                    'Dimension': dim,
                    'Loading': f"{loading:+.3f}",
                    'Contribution': "Strong" if abs(loading) > 0.4 else "Moderate" if abs(loading) > 0.2 else "Weak"
                })
            
            st.table(loading_data)
            
            st.caption("""
            **Loadings** show how much each dimension contributes to the overall index.
            Higher absolute values mean stronger contribution.
            """)
        
        # Key finding (updated)
        st.success(f"""
        **üéØ Key Finding: TikTok Confirmed as Extreme Outlier**
        
        With the PCA-weighted index, TikTok scores +{pca_index['platforms']['TikTok']['pca_index']:.2f}, 
        which is {pca_index['platforms']['TikTok']['pca_index'] / pca_index['platforms']['Meta']['pca_index']:.1f}x 
        higher than Meta. This reflects not just syntactic complexity but also 
        aggressive use of discretion language ("sole discretion", "at any time", etc.).
        """)
        
        # Quick ITS insight (if data available)
        if its_analysis and 'Meta' in its_analysis:
            meta_gdpr = its_analysis['Meta']['events'].get('GDPR Effective (May 2018)', {})
            if meta_gdpr:
                st.info(f"""
                üìà **Regulatory Impact Preview**: After GDPR, Meta's legalization trend 
                accelerated by {meta_gdpr.get('slope_change', 0):+.4f}/year. 
                See the **Time Series** tab for full analysis.
                """)
        
        # Comparison section
        with st.expander("üìä Compare Old vs New Index"):
            st.markdown("**Simple Average vs PCA-Weighted**")
            
            comparison_data = []
            for platform in selected_platforms:
                if platform in platform_scores and platform in pca_index['platforms']:
                    # Old index (simple average)
                    dim = platform_scores[platform]
                    old_index = (dim['complexity_score'] + dim['agency_score'] + dim['formality_score']) / 3
                    
                    # New index (PCA)
                    new_index = pca_index['platforms'][platform]['pca_index']
                    
                    comparison_data.append({
                        'Platform': platform,
                        'Simple Average': round(old_index, 3),
                        'PCA-Weighted': round(new_index, 3),
                        'Difference': round(new_index - old_index, 3)
                    })
            
            df_compare = pd.DataFrame(comparison_data)
            df_compare = df_compare.sort_values('PCA-Weighted', ascending=False)
            st.dataframe(df_compare, use_container_width=True)
            
            st.caption("""
            The PCA-weighted index incorporates a 4th dimension (Discretion) and uses
            data-driven weights instead of equal weighting. This typically amplifies
            differences between platforms.
            """)
    else:
        st.warning("PCA Index data not available. Please run calculate_pca_index.py.")

# TAB 2: DIMENSIONS
with tab2:
    st.subheader("Four Dimensions of Legalization")

    # Direction explanation
    st.info("""
    **üìê How to Read This Chart:**

    All four dimensions point in the **same direction** ‚Äî larger area = more **platform-favorable** (less user-friendly):

    | Dimension | Higher Value Means | Impact on Users |
    |-----------|-------------------|-----------------|
    | **Complexity** | Harder to read (longer sentences, deeper syntax) | ‚ùå Harder to understand |
    | **Agency Asymmetry** | More "platform" mentions vs "user" mentions | ‚ùå Platform-centric language |
    | **Formality** | More legal jargon and formal terms | ‚ùå Less accessible |
    | **Discretion** | More discretionary power language | ‚ùå More platform control |

    ‚ö†Ô∏è **Larger radar area = More platform-protective policy = Worse for users**
    """)

    # Radar chart with 4 dimensions
    fig = go.Figure()
    
    categories = ['Complexity', 'Agency Asymmetry', 'Formality', 'Discretion']  # Added Discretion
    
    for platform in selected_platforms:
        # Get scores from dimension_scores and discretion_scores
        dim = platform_scores[platform]
        disc = discretion_scores.get(platform, {})
        discretion_val = disc.get('avg_discretion_ratio', 0)

        # Dynamic normalization based on actual data range
        if discretion_scores and discretion_val:
            all_ratios = [d.get('avg_discretion_ratio', 0) for d in discretion_scores.values()]
            ratio_min = min(all_ratios)
            ratio_max = max(all_ratios)
            ratio_mean = sum(all_ratios) / len(all_ratios) if all_ratios else 1

            # Z-score-like normalization to match other dimensions
            # Avoid division by zero
            denom = (ratio_max - ratio_min) if (ratio_max - ratio_min) > 0 else 1
            discretion_normalized = (discretion_val - ratio_mean) / denom * 2
        else:
            discretion_normalized = 0
        
        # Use pre-calculated radar scores for other dimensions if available, or raw
        # viz_data['radar'] has the normalized scores.
        # But viz_data['radar'] only has 3 dimensions.
        # We need to reconstruct the list.
        
        existing_radar = viz_data['radar'][platform]
        # existing_radar is [complexity, agency, formality]
        
        scores = existing_radar[:3] + [discretion_normalized]
        scores_closed = scores + [scores[0]]
        
        fig.add_trace(go.Scatterpolar(
            r=scores_closed,
            theta=categories + [categories[0]],
            fill='toself',
            name=platform,
            line_color=PLATFORM_CONFIG[platform]['color']
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[-1, 1.5]
            )
        ),
        showlegend=True,
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Updated explanation
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        **Complexity** (Syntactic)
        - Average sentence length
        - Parse tree depth
        - Clause nesting

        **Agency Asymmetry** (Pronoun Ratio)
        - Platform pronouns: we, our, us
        - User pronouns: you, your
        - Ratio = (platform + 1) / (user + 1)
        """)

    with col2:
        st.markdown("""
        **Formality** (Legal Term Density)
        - Terms: hereby, herein, notwithstanding, pursuant to, etc.
        - Count per 1,000 words
        - 16 formal legal terms tracked

        **Discretion** (Power Language Ratio)
        - Discretion: sole discretion, at any time, reserve the right
        - Disclaimer: as is, without liability, no warranty
        - Ratio = (discretion + disclaimer) / obligation
        """)

# TAB 3: AGENCY ASYMMETRY
with tab3:
    st.subheader("Agency Visibility: Platform vs User")
    
    # Filter agency comparison data
    filtered_agency = [
        item for item in viz_data['agency_comparison'] 
        if item['platform'] in selected_platforms
    ]
    
    # Create grouped bar chart
    fig = px.bar(
        filtered_agency,
        x='platform',
        y='visibility',
        color='entity',
        barmode='group',
        color_discrete_map={'User': '#10B981', 'Platform': '#EF4444'},
        labels={'visibility': 'Visibility Rate', 'platform': 'Platform'},
        title="Explicit Agency Visibility by Entity Type"
    )
    
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True)
    
    # Calculate asymmetry stats
    st.subheader("Asymmetry Statistics")
    
    asymmetry_data = []
    for platform in selected_platforms:
        scores = platform_scores[platform]
        asymmetry_data.append({
            'Platform': platform,
            'Platform Visibility': f"{scores['raw_metrics']['platform_visibility']:.1%}",
            'User Visibility': f"{scores['raw_metrics']['user_visibility']:.1%}",
            'Asymmetry Factor': f"{scores['raw_metrics']['asymmetry_factor']:.1f}x"
        })
    
    st.table(asymmetry_data)
    
    st.warning("""
    **‚ö†Ô∏è Interpretation:** USER actions are explicit 79-81% of the time, while 
    PLATFORM actions are explicit only 5-7% of the time. This 11-20x asymmetry 
    suggests systematic use of passive voice and nominalization to obscure 
    platform obligations.
    """)

# NEW TAB 4: DISCRETION ANALYSIS
with tab4:
    st.subheader("Platform Discretion Language")
    
    st.markdown("""
    **What is Discretion Language?**
    
    Discretion language gives platforms unilateral power to act without user consent.
    Examples: "sole discretion", "reserves the right", "at any time", "without notice"
    """)
    
    # Filter to selected platforms
    filtered_discretion = {p: discretion_scores[p] for p in selected_platforms if p in discretion_scores}
    
    if filtered_discretion:
        # Simpler: just discretion ratio
        ratio_data = []
        for platform, data in filtered_discretion.items():
            ratio_data.append({
                'Platform': platform,
                'Discretion Ratio': data['avg_discretion_ratio'],
                'Power Index': data['avg_platform_power_index']
            })
        
        df_ratio = pd.DataFrame(ratio_data)
        df_ratio = df_ratio.sort_values('Discretion Ratio', ascending=False)
        
        fig_disc = px.bar(
            df_ratio,
            x='Platform',
            y='Discretion Ratio',
            color='Platform',
            color_discrete_map={p: PLATFORM_CONFIG[p]['color'] for p in PLATFORM_CONFIG},
            title="Discretion Ratio by Platform"
        )
        
        fig_disc.update_layout(
            yaxis_title="Discretion Ratio (Platform Power / User Obligation)",
            showlegend=False,
            height=400
        )
        
        st.plotly_chart(fig_disc, use_container_width=True)
        
        # Detailed metrics table
        st.subheader("Detailed Discretion Metrics")
        
        detail_data = []
        for platform, data in filtered_discretion.items():
            detail_data.append({
                'Platform': platform,
                'Discretion/1k': f"{data['avg_discretion_per_1k']:.2f}",
                'Disclaimer/1k': f"{data['avg_disclaimer_per_1k']:.2f}",
                'Obligation/1k': f"{data['avg_obligation_per_1k']:.2f}",
                'Discretion Ratio': f"{data['avg_discretion_ratio']:.2f}",
                'Power Index': f"{data['avg_platform_power_index']:.2f}"
            })
        
        st.table(detail_data)
        
        # Explanation
        st.markdown("""
        **Metrics Explained:**
        - **Discretion/1k**: Discretion expressions per 1,000 words
        - **Disclaimer/1k**: Liability disclaimers per 1,000 words
        - **Obligation/1k**: User obligation language per 1,000 words
        - **Discretion Ratio**: (Discretion + Disclaimer) / Obligation
        - **Power Index**: Combined platform authority measure
        
        **Higher values = More platform-favorable language**
        """)
        
        # Top expressions found
        # Top expressions found
        with st.expander("üîç Common Discretion Expressions (Corpus-Derived)"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **Discretion Terms:**
                - "at any time" (113x)
                - "reserve the right" (90x)
                - "sole discretion" (56x)
                - "for any reason" (28x)
                - "without notice" (23x)
                """)
            
            with col2:
                st.markdown("""
                **Disclaimer Terms:**
                - "without limitation" (32x)
                - "as is" / "as available"
                - "not responsible for"
                - "limitation of liability"
                - "indemnify"
                """)
            
            st.caption("""
            These expressions were extracted directly from the 50 Tier 1 policy documents,
            not from a generic predefined list. This ensures platform-specific language is captured.
            """)
    else:
        st.warning("Discretion scores not available. Please run calculate_discretion.py.")

# TAB 5: DETAILED METRICS
with tab5:
    st.subheader("Detailed Metrics Table")
    
    # Filter detailed metrics
    filtered_metrics = [
        m for m in viz_data['detailed_metrics'] 
        if m['platform'] in selected_platforms
    ]
    
    # Format for display
    df = pd.DataFrame(filtered_metrics)
    df = df.sort_values('legalization_index', ascending=False)
    
    # Format columns
    df['legalization_index'] = df['legalization_index'].apply(lambda x: f"{x:+.3f}")
    df['complexity'] = df['complexity'].apply(lambda x: f"{x:+.3f}")
    df['agency'] = df['agency'].apply(lambda x: f"{x:+.3f}")
    df['formality'] = df['formality'].apply(lambda x: f"{x:+.3f}")
    df['adl'] = df['adl'].apply(lambda x: f"{x:.2f}")
    df['platform_visibility'] = df['platform_visibility'].apply(lambda x: f"{x:.1%}")
    df['user_visibility'] = df['user_visibility'].apply(lambda x: f"{x:.1%}")
    df['asymmetry'] = df['asymmetry'].apply(lambda x: f"{x:.1f}x")
    
    # Rename columns
    df = df.rename(columns={
        'platform': 'Platform',
        'legalization_index': 'Policy Index',
        'complexity': 'Complexity',
        'agency': 'Agency',
        'formality': 'Formality',
        'adl': 'ADL',
        'platform_visibility': 'Platform Vis',
        'user_visibility': 'User Vis',
        'asymmetry': 'Asymmetry',
        'doc_count': 'Docs'
    })
    
    st.dataframe(df, use_container_width=True, hide_index=True)
    
    # CSV Export Button
    st.subheader("Export Data")

    # Prepare export data
    export_data = []
    for platform in selected_platforms:
        if platform in platform_scores:
            row = {
                'Platform': platform,
                'Policy Index (PCA)': pca_index['platforms'].get(platform, {}).get('pca_index', 'N/A') if pca_index else 'N/A',
                'Rank': pca_index['platforms'].get(platform, {}).get('rank', 'N/A') if pca_index else 'N/A',
                'Complexity Score': platform_scores[platform].get('complexity_score', 'N/A'),
                'Agency Score': platform_scores[platform].get('agency_score', 'N/A'),
                'Formality Score': platform_scores[platform].get('formality_score', 'N/A'),
            }
            
            # Add discretion metrics if available
            if platform in discretion_scores:
                row['Discretion Ratio'] = discretion_scores[platform].get('avg_discretion_ratio', 'N/A')
                row['Power Index'] = discretion_scores[platform].get('avg_platform_power_index', 'N/A')
            
            export_data.append(row)

    df_export = pd.DataFrame(export_data)

    csv = df_export.to_csv(index=False)
    st.download_button(
        label="üì• Download Full Metrics (CSV)",
        data=csv,
        file_name="policy_explorer_metrics.csv",
        mime="text/csv"
    )

# TAB 6: TIME SERIES ANALYSIS
with tab6:
    st.subheader("Policy Evolution Over Time")
    
    # Load hybrid temporal data
    from datetime import datetime

    current_file = Path(__file__).resolve()
    dashboard_dir = current_file.parent
    data_dir = dashboard_dir / 'data'

    hybrid_path = data_dir / 'tier1_hybrid_temporal.json'
    
    with open(hybrid_path) as f:
        hybrid_temporal = json.load(f)
    
    st.success("""
    üìä **All Platforms Have Real Temporal Data** (Verified 2026-01-22)

    | Platform | Documents | Year Range |
    |----------|-----------|------------|
    | Meta | 354 | 2005-2025 (21 years) |
    | Reddit | 232 | 2007-2025 (19 years) |
    | YouTube | 177 | 2013-2025 (13 years) |
    | WhatsApp | 167 | 2009-2025 (17 years) |
    | LinkedIn | 163 | 2013-2025 (13 years) |
    | TikTok | 95 | 2019-2025 (7 years) |

    Data source: TransparencyDB (transparencydb_dev.documents.json)
    """)
    
    st.markdown("---")
    
    # Platform filter for time series - only show platforms with temporal data
    temporal_platforms = list(hybrid_temporal.get('platforms', {}).keys())
    ts_platform_filter = st.multiselect(
        "Select platforms to display",
        options=temporal_platforms,
        default=temporal_platforms,
        key='ts_filter'
    )

    # Policy Index Definition
    with st.expander("üìñ **What is Policy Index?**"):
        st.markdown("""
        **Policy Index** (= Legalization Index) measures how "legalized" and platform-protective a policy document is.

        **Formula:**
        ```
        Policy Index = (Complexity + Agency + Formality) / 3
        ```

        **Components (4 Dimensions, PCA-weighted):**
        - **Complexity**: Syntactic complexity (avg sentence length, tree depth)
        - **Agency Asymmetry**: Platform vs user pronoun ratio (we/our vs you/your)
        - **Formality**: Formal legal terms density (hereby, pursuant to, notwithstanding, etc.)
        - **Discretion**: Platform power language ratio (discretion + disclaimer) / obligation

        **Interpretation:**
        - **Higher values** ‚Üí More platform-protective, legally complex language
        - **Lower values** ‚Üí More user-friendly, accessible language
        - **Increase over time** ‚Üí "Legalization" trend (policies becoming more protective)
        """)

    # Create time series chart
    fig_ts = go.Figure()

    for platform in ts_platform_filter:
        if platform not in hybrid_temporal['platforms']:
            continue
        platform_data = hybrid_temporal['platforms'][platform]
        time_series = platform_data['time_series']
        
        # Extract dates and indices
        platform_dates = [datetime.fromisoformat(t['date']) for t in time_series]
        platform_indices = [t['index'] for t in time_series]
        
        # Different styles for real vs illustrative
        is_real = platform_data['type'] == 'real'
        
        fig_ts.add_trace(go.Scatter(
            x=platform_dates,
            y=platform_indices,
            mode='lines+markers' if is_real else 'lines',
            name=f"{platform} {'‚úì' if is_real else '~'}",
            line=dict(
                color=PLATFORM_CONFIG[platform]['color'],
                width=3 if is_real else 2,
                dash='solid' if is_real else 'dash'
            ),
            marker=dict(size=6 if is_real else 0),
            hovertemplate=(
                f"<b>{platform}</b><br>" +
                "Date: %{x|%Y-%m-%d}<br>" +
                "Index: %{y:.3f}<br>" +
                f"<i>{'Real data' if is_real else 'Illustrative'}</i><br>" +
                "<extra></extra>"
            )
        ))
    
    # Add regulatory event markers with Announced vs Effective distinction
    # Define styling for different event types
    event_styles = {
        'Announced': {'color': 'rgba(100, 100, 200, 0.5)', 'dash': 'dot', 'width': 1.5},
        'Signed': {'color': 'rgba(100, 100, 200, 0.5)', 'dash': 'dot', 'width': 1.5},
        'Effective': {'color': 'rgba(200, 50, 50, 0.7)', 'dash': 'solid', 'width': 2},
    }

    for event_name, event_date in hybrid_temporal['regulatory_events'].items():
        event_dt = datetime.fromisoformat(event_date)

        # Determine event type (Announced/Signed vs Effective)
        event_type = 'Effective'  # default
        for key in event_styles.keys():
            if key in event_name:
                event_type = key
                break

        style = event_styles[event_type]

        # Use add_shape for the line (more robust than add_vline for datetimes)
        fig_ts.add_shape(
            type="line",
            x0=event_dt,
            y0=0,
            x1=event_dt,
            y1=1,
            xref="x",
            yref="paper",
            line=dict(
                color=style['color'],
                width=style['width'],
                dash=style['dash']
            )
        )

        # Create label with type indicator
        regulation = event_name.split()[0]  # GDPR, CCPA, etc.
        type_short = "üì¢" if event_type in ['Announced', 'Signed'] else "‚ö°"
        label = f"{regulation} {type_short}"

        # Use add_annotation for the text
        fig_ts.add_annotation(
            x=event_dt,
            y=1.02,
            xref="x",
            yref="paper",
            text=label,
            showarrow=False,
            font=dict(size=9, color='rgba(50,50,50,0.8)'),
            textangle=-90,
            xanchor="center",
            yanchor="bottom"
        )

    # Add legend explanation for regulatory lines
    fig_ts.add_annotation(
        x=0.99,
        y=0.02,
        xref="paper",
        yref="paper",
        text="üì¢ = Announced/Signed | ‚ö° = Effective",
        showarrow=False,
        font=dict(size=10),
        bgcolor="rgba(255,255,255,0.8)",
        xanchor="right"
    )
    
    fig_ts.update_layout(
        title="Platform Policy Index Evolution (2006-2025)",
        xaxis_title="Date",
        yaxis_title="Policy Index",
        height=600,
        hovermode='x unified',
        legend=dict(
            orientation="v",
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=0.01,
            bgcolor="rgba(255, 255, 255, 0.8)"
        ),
        plot_bgcolor='rgba(240, 240, 240, 0.3)'
    )
    
    st.plotly_chart(fig_ts, use_container_width=True)

    # NEW: Discretion Ratio Time Series (Lexicon-based)
    st.markdown("---")
    st.subheader("üìä Discretion Ratio Over Time (Lexicon-Based)")

    with st.expander("üìñ **What is Discretion Ratio?**"):
        st.markdown("""
        **Discretion Ratio** measures the balance of power language between platform and user.

        **Formula:**
        ```
        Discretion Ratio = (Discretion + Disclaimer) / max(Obligation, 1)
        ```

        **Where:**
        - **Discretion**: 22 terms like "sole discretion", "at any time", "reserve the right"
        - **Disclaimer**: 13 terms like "without limitation", "as is", "no warranty"
        - **Obligation**: 11 terms like "you must", "you agree", "you shall"

        **Interpretation:**
        - **Ratio > 1** ‚Üí Platform talks more about its own power than user obligations
        - **Ratio < 1** ‚Üí More balanced or user-protective
        - **Increase over time** ‚Üí Platform gaining more discretionary power
        """)

    # Load discretion scores and build temporal discretion data
    discretion_path = data_dir / 'tier1_discretion_scores.json'
    try:
        with open(discretion_path) as f:
            disc_scores = json.load(f)

        # Build doc_id to discretion mapping
        doc_disc_map = {}
        for platform, pdata in disc_scores.items():
            if 'documents' in pdata:
                for doc in pdata['documents']:
                    doc_id = doc.get('doc_id')
                    if doc_id:
                        doc_disc_map[doc_id] = {
                            'discretion_ratio': doc.get('discretion_ratio', 0),
                            'platform_power_index': doc.get('platform_power_index', 0),
                        }

        # Create Discretion Ratio time series chart
        fig_disc_ts = go.Figure()

        for platform in ts_platform_filter:
            if platform not in hybrid_temporal['platforms']:
                continue
            platform_data = hybrid_temporal['platforms'][platform]
            time_series = platform_data['time_series']

            # Extract dates and discretion ratios
            dates = []
            disc_ratios = []
            for t in time_series:
                try:
                    dt = datetime.fromisoformat(t['date'])
                    doc_id = t.get('doc_id')
                    disc_data = doc_disc_map.get(doc_id, {})
                    ratio = disc_data.get('discretion_ratio', 0)

                    # Use platform average if no doc-level data
                    if ratio == 0:
                        ratio = disc_scores.get(platform, {}).get('avg_discretion_ratio', 1.5)

                    dates.append(dt)
                    disc_ratios.append(ratio)
                except:
                    continue

            if dates and disc_ratios:
                color = PLATFORM_CONFIG.get(platform, {}).get('color', '#888888')
                fig_disc_ts.add_trace(go.Scatter(
                    x=dates,
                    y=disc_ratios,
                    mode='lines+markers',
                    name=platform,
                    line=dict(color=color, width=2),
                    marker=dict(size=6, color=color),
                ))

        # Add regulatory event lines
        for event_name, event_date in hybrid_temporal['regulatory_events'].items():
            event_dt = datetime.fromisoformat(event_date)
            event_type = 'Effective'
            for key in ['Announced', 'Signed', 'Effective']:
                if key in event_name:
                    event_type = key
                    break

            line_style = 'dot' if event_type in ['Announced', 'Signed'] else 'solid'
            line_color = 'rgba(100,100,200,0.5)' if event_type in ['Announced', 'Signed'] else 'rgba(200,50,50,0.7)'

            fig_disc_ts.add_shape(
                type="line", x0=event_dt, y0=0, x1=event_dt, y1=1,
                xref="x", yref="paper",
                line=dict(color=line_color, width=1.5, dash=line_style)
            )

        fig_disc_ts.update_layout(
            title="Discretion Ratio Evolution (Lexicon-Based Analysis)",
            xaxis_title="Date",
            yaxis_title="Discretion Ratio",
            height=500,
            hovermode='x unified',
            legend=dict(orientation="v", yanchor="top", y=0.99, xanchor="left", x=0.01),
            plot_bgcolor='rgba(240, 240, 240, 0.3)',
            yaxis=dict(range=[0, 5])
        )

        st.plotly_chart(fig_disc_ts, use_container_width=True)

        # Summary comparison
        st.info("""
        **üìä Two Complementary Perspectives:**

        | Metric | Measures | Source |
        |--------|----------|--------|
        | **Policy Index** | Document structure complexity | Syntax analysis (ADL, tree depth) |
        | **Discretion Ratio** | Power language balance | Lexicon analysis (72 discretion terms) |

        Both metrics increasing = Platform policies becoming more **complex** AND more **power-asymmetric**.
        """)

    except FileNotFoundError:
        st.warning("Discretion scores data not available. Please run the analysis script.")

    # NEW: ITS Analysis Results Section
    st.markdown("---")
    st.subheader("üìà Regulatory Impact Analysis (ITS)")
    
    if its_analysis:
        for platform, platform_its in its_analysis.items():
            with st.expander(f"üìà {platform} - Regulatory Event Impact"):
                for event_name, event_stats in platform_its['events'].items():
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        pre_trend = event_stats['pre_trend']['slope_per_year']
                        pre_sig = "‚úì" if event_stats['pre_significant'] else ""
                        st.metric(
                            f"Pre-{event_name.split()[0]} Trend",
                            f"{pre_trend:+.4f}/yr",
                            delta=pre_sig + " significant" if event_stats['pre_significant'] else "not significant",
                            delta_color="normal"
                        )
                    
                    with col2:
                        post_trend = event_stats['post_trend']['slope_per_year']
                        post_sig = "‚úì" if event_stats['post_significant'] else ""
                        st.metric(
                            f"Post-{event_name.split()[0]} Trend",
                            f"{post_trend:+.4f}/yr",
                            delta=post_sig + " significant" if event_stats['post_significant'] else "not significant"
                        )
                    
                    with col3:
                        if event_stats['level_change']:
                            st.metric(
                                "Immediate Impact",
                                f"{event_stats['level_change']:+.3f}",
                                delta="level jump" if event_stats['level_change'] > 0 else "level drop"
                            )
        
        # Summary callout
        st.success("""
        **üéØ Key ITS Findings**
        
        - **GDPR (2018)**: Statistically significant acceleration in legalization trend for Meta
        - **CCPA (2020)**: Secondary impact, less pronounced than GDPR
        - **Overall Pattern**: Regulatory events correlate with increased policy complexity
        
        *Note: ITS analysis performed only on platforms with real temporal data (Meta, YouTube)*
        """)
    else:
        st.info("ITS analysis data not available.")
    
    # Key observations
    st.subheader("Temporal Trends")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "Meta Evolution",
            "2006-2025",
            "+0.40 index points (19 years)"
        )
    
    with col2:
        st.metric(
            "GDPR Impact",
            "2018",
            "Visible acceleration in all platforms"
        )
    
    with col3:
        st.metric(
            "YouTube Evolution",
            "2007-2025",
            "+0.40 index points (18 years)"
        )
    
    st.markdown("---")
    
    st.subheader("Major Regulatory Milestones")
    
    st.markdown("""
    **Key Regulatory Events:**
    - **GDPR** (2016 announced, 2018 effective): EU General Data Protection Regulation  
      ‚Üí Visible acceleration in legalization for Meta and YouTube post-2016
    - **CCPA** (2018 signed, 2020 effective): California Consumer Privacy Act  
      ‚Üí Continued complexity increases through 2020
    - **CPRA** (2023): California Privacy Rights Act  
      ‚Üí Recent policy updates reflect enhanced consumer rights
    - **DSA** (2024): Digital Services Act (EU)  
      ‚Üí Latest regulatory milestone affecting platform policies
    
    **Observed Patterns:**
    - üìà **Steady Increase**: Meta and YouTube show consistent 19-year upward trend
    - ‚ö° **Regulatory Acceleration**: Growth rate increases around GDPR/CCPA deadlines
    - üîÑ **Industry-Wide**: Both platforms follow similar trajectories despite different starting points
    - üìä **Convergence**: Gap between Meta (+0.279) and YouTube (-0.134) reflects baseline differences, not temporal divergence
    """)
    
    # Data quality note
    st.subheader("Data Quality & Methodology")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Real Data (Meta & YouTube):**
        - Source: 1,337 policy versions
        - Sampling: Quarterly representative documents
        - Coverage: 128 Meta docs, 1,209 YouTube docs
        - Index Estimation: Based on current complexity metrics adjusted for temporal position relative to regulatory events
        """)
    
    with col2:
        st.markdown("""
        **Illustrative Patterns:**
        - Calibrated to 2024 measured index
        - GDPR/CCPA jumps modeled
        - Platform-specific growth rates
        - Conservative assumptions
        - Labeled clearly as estimated
        """)
    
    st.info("""
    üí° **Research Implications**: The 19-year Meta and YouTube datasets enable:
    - Causal analysis of regulatory impact (GDPR as natural experiment)
    - Lead/lag identification between policy changes and regulations
    - Event study methodology validation
    - Predictive modeling of future legalization trends
    
    This represents one of the longest temporal policy datasets in platform governance research.
    """)

# TAB 7: REGULATORY EVASION ANALYSIS
with tab7:
    st.subheader("Regulatory Evasion Pattern Analysis")

    st.markdown("""
    **Research Question**: Did platforms genuinely comply with GDPR/CCPA, or did they adapt their language
    while preserving power asymmetry?
    """)

    # Load regulatory evasion data
    evasion_path = data_dir / 'all_platforms_regulatory_analysis.json'

    try:
        with open(evasion_path) as f:
            evasion_data = json.load(f)

        # Get metadata if available
        metadata = evasion_data.pop('_metadata', {})
        real_data_platforms = metadata.get('data_sources', {}).get('tier1_extracted', [])
        estimated_platforms = metadata.get('data_sources', {}).get('metadata_estimated', [])

        st.success(f"""
        ‚úÖ **Real Text Analysis: {len(evasion_data)} Platforms** (Updated {metadata.get('last_updated', '2026-02-04')[:10]})

        All platforms analyzed using actual policy document text from tier1_extracted.json:
        Meta, YouTube, WhatsApp, Reddit, LinkedIn, TikTok, Twitter, Instagram, Pinterest, Discord
        """)

        # Key finding callout - Multiple platforms with evasion patterns
        st.error("""
        **‚ö†Ô∏è Key Finding: Multiple Platforms Show Evasion Patterns**

        **Reddit** (Most Aggressive - Increased ALL metrics):
        - `sole_discretion`: **+144%** | `may_terminate`: **+174%** | `at_any_time`: **+38%**

        **Meta** (Substitution Pattern - Reduced explicit, added implicit):
        - `may_terminate`: **+76%** ‚Üë (new pattern introduced)
        - `sole_discretion`: -60% | `reserve_right`: -88% (reduced explicit language)

        **Twitter/X** (Post-CCPA Introduction - No pre-GDPR data for some patterns):
        - `reserve_right`: **+100%** (0‚Üí0.18) | `without_liability`: **+100%** (0‚Üí0.21)
        - `may_terminate`: **+100%** (0‚Üí0.08) - all new patterns added post-regulation

        **LinkedIn** (Partial Evasion):
        - `without_liability`: **+10.6%** ‚Üë (only increasing pattern)
        - Other patterns reduced but liability limitation strengthened
        """)

        st.markdown("---")

        # Pattern comparison chart
        st.subheader("Discretion Language Changes: Pre-GDPR vs Post-CCPA")

        # Prepare data for visualization
        pattern_data = []
        for platform, data in evasion_data.items():
            if 'patterns' in data:
                for pattern, values in data['patterns'].items():
                    pattern_data.append({
                        'Platform': platform,
                        'Pattern': pattern.replace('_', ' ').title(),
                        'Pre-GDPR': values['pre_gdpr'],
                        'Post-CCPA': values['post_ccpa'],
                        'Change %': values['change_pct']
                    })

        if pattern_data:
            import pandas as pd
            df_evasion = pd.DataFrame(pattern_data)

            # Filter to key patterns
            key_patterns = ['Sole Discretion', 'May Terminate', 'Reserve Right', 'For Any Reason']
            df_key = df_evasion[df_evasion['Pattern'].isin(key_patterns)]

            # Create grouped bar chart
            fig_evasion = go.Figure()

            platforms = df_key['Platform'].unique()
            patterns = df_key['Pattern'].unique()

            colors = {'Pre-GDPR': '#3498db', 'Post-CCPA': '#e74c3c'}

            for period in ['Pre-GDPR', 'Post-CCPA']:
                fig_evasion.add_trace(go.Bar(
                    name=period,
                    x=[f"{row['Platform']}<br>{row['Pattern']}" for _, row in df_key.iterrows()],
                    y=df_key[period],
                    marker_color=colors[period]
                ))

            fig_evasion.update_layout(
                barmode='group',
                title="Discretion Language Frequency (per 1,000 words)",
                yaxis_title="Frequency per 1k words",
                height=500,
                legend=dict(orientation="h", yanchor="bottom", y=1.02)
            )

            st.plotly_chart(fig_evasion, use_container_width=True)

            # Change percentage table
            st.subheader("Change Analysis by Platform")

            # Pivot for display
            pivot_df = df_key.pivot(index='Platform', columns='Pattern', values='Change %')

            # Style the dataframe
            def color_change(val):
                if pd.isna(val):
                    return ''
                if val > 50:
                    return 'background-color: #ffcccc'  # Red for big increase
                elif val < -50:
                    return 'background-color: #ccffcc'  # Green for big decrease
                return ''

            st.dataframe(
                pivot_df.style.map(color_change).format("{:.0f}%"),
                use_container_width=True
            )

            st.caption("""
            üî¥ Red = Increased discretion (potential evasion)
            üü¢ Green = Decreased discretion (compliance)
            """)

        # Platform classification
        st.markdown("---")
        st.subheader("Platform Compliance Classification")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("""
            **‚úÖ Genuine Compliance**
            - **WhatsApp**: All patterns reduced (avg -80%)
            - **YouTube**: Major patterns reduced (avg -90%)

            *Substantial reduction in discretion language*
            """)

        with col2:
            st.markdown("""
            **‚ö†Ô∏è Substitution Strategy**
            - **Meta**: +76% `may_terminate` (offsetting -88% `reserve_right`)
            - **Twitter**: New patterns introduced post-CCPA
            - **LinkedIn**: +10.6% `without_liability`

            *Reduced explicit, introduced implicit equivalents*
            """)

        with col3:
            st.markdown("""
            **‚ùå Regulatory Defiance**
            - **Reddit**: ALL patterns increased
                - +174% `may_terminate`
                - +144% `sole_discretion`
                - +38% `at_any_time`

            *Openly increased discretion post-GDPR/CCPA*
            """)

        # Interpretation
        st.markdown("---")
        st.info("""
        **üìä Interpretation: Three Distinct Regulatory Response Strategies**

        **1. Genuine Compliance** (WhatsApp, YouTube):
        - Substantially reduced ALL discretion patterns (-80% to -90%)
        - No compensatory introduction of new patterns
        - Net effect: Actual power rebalancing toward users

        **2. Substitution/Evasion** (Meta, Twitter, LinkedIn):
        - Removed explicit language ("sole discretion", "reserve the right")
        - Introduced functionally equivalent patterns ("may terminate", "without liability")
        - **Net effect**: Power asymmetry maintained through language evolution

        **3. Regulatory Defiance** (Reddit):
        - INCREASED discretion language across ALL patterns post-GDPR/CCPA
        - Suggests deliberate non-compliance or different regulatory interpretation
        - **Net effect**: Power asymmetry explicitly strengthened

        **Implication**: Regulation success is non-uniform. Language-based analysis reveals that approximately
        30% of platforms show evasion behavior that text-mining can detect but surface compliance audits may miss.
        """)

    except FileNotFoundError:
        st.warning("Regulatory evasion analysis data not found. Please run the analysis script first.")

# TAB 8: PRIVACY VS TOS SEPARATION
with tab8:
    st.subheader("üìÑ Document Type Separation Analysis")

    st.markdown("""
    **Critical Methodological Insight**: Mixing Privacy Policy and Terms of Service
    produces misleading results. This tab shows analysis **separated by document type**.
    """)

    # Load doc type analysis
    doc_type_path = data_dir / 'doc_type_analysis.json'

    try:
        with open(doc_type_path) as f:
            doc_type_data = json.load(f)

        st.success("""
        ‚úÖ **Document Type Analysis Available**

        GDPR primarily targets **Privacy Policies**, not Terms of Service.
        Separating these reveals dramatically different patterns.
        """)

        # Summary comparison table
        st.subheader("Privacy Policy vs Terms of Service: Change Comparison")

        comparison_rows = []
        for platform in ['Meta', 'YouTube', 'WhatsApp', 'Reddit', 'LinkedIn', 'Twitter', 'Pinterest']:
            if platform not in doc_type_data:
                continue

            priv_change = doc_type_data[platform].get('privacy', {}).get('change_pct', None)
            tos_change = doc_type_data[platform].get('tos', {}).get('change_pct', None)

            comparison_rows.append({
                'Platform': platform,
                'Privacy Œî': f"{priv_change:+.1f}%" if priv_change is not None else "N/A",
                'ToS Œî': f"{tos_change:+.1f}%" if tos_change is not None else "N/A",
            })

        if comparison_rows:
            import pandas as pd
            df_compare = pd.DataFrame(comparison_rows)

            # Style the dataframe
            def style_change(val):
                if val == "N/A":
                    return 'color: gray'
                try:
                    num = float(val.replace('%', '').replace('+', ''))
                    if num > 50:
                        return 'background-color: #ffcccc; color: darkred; font-weight: bold'
                    elif num < -30:
                        return 'background-color: #ccffcc; color: darkgreen'
                except:
                    pass
                return ''

            st.dataframe(
                df_compare.style.map(style_change, subset=['Privacy Œî', 'ToS Œî']),
                use_container_width=True,
                hide_index=True
            )

        st.caption("""
        üî¥ Red = Large increase (potential defensive response)
        üü¢ Green = Decrease (compliance)
        """)

        # Key insights
        st.markdown("---")
        st.subheader("üîç Key Insights from Separation")

        col1, col2 = st.columns(2)

        with col1:
            st.error("""
            **Meta: Opposite Patterns**

            - Privacy Policy: **+1,281%** ‚Üë
            - Terms of Service: **-25%** ‚Üì

            Meta dramatically increased discretion in Privacy
            (defensive response to GDPR scrutiny) while
            reducing it in ToS.
            """)

        with col2:
            st.success("""
            **Reddit: Reinterpretation Needed**

            - Privacy Policy: **-63%** ‚Üì
            - ToS: N/A (insufficient pre-GDPR data)

            Previously labeled as "regulatory defiance,"
            Reddit actually shows **strong privacy compliance**
            when separated by document type.
            """)

        # Visualization: Bar chart comparison
        st.markdown("---")
        st.subheader("Discretion Ratio Change by Document Type")

        fig_doctype = go.Figure()

        platforms_with_data = []
        priv_changes = []
        tos_changes = []

        for platform in ['Meta', 'YouTube', 'WhatsApp', 'Reddit', 'LinkedIn', 'Twitter', 'Pinterest']:
            if platform not in doc_type_data:
                continue

            priv = doc_type_data[platform].get('privacy', {}).get('change_pct', None)
            tos = doc_type_data[platform].get('tos', {}).get('change_pct', None)

            if priv is not None or tos is not None:
                platforms_with_data.append(platform)
                priv_changes.append(priv if priv is not None else 0)
                tos_changes.append(tos if tos is not None else 0)

        fig_doctype.add_trace(go.Bar(
            name='Privacy Policy',
            x=platforms_with_data,
            y=priv_changes,
            marker_color='#3498db'
        ))

        fig_doctype.add_trace(go.Bar(
            name='Terms of Service',
            x=platforms_with_data,
            y=tos_changes,
            marker_color='#e74c3c'
        ))

        fig_doctype.add_hline(y=0, line_dash="solid", line_color="black", line_width=1)

        fig_doctype.update_layout(
            title="Discretion Ratio Change: Pre-GDPR to Post-CCPA",
            yaxis_title="Change (%)",
            barmode='group',
            height=500,
            legend=dict(orientation="h", yanchor="bottom", y=1.02)
        )

        st.plotly_chart(fig_doctype, use_container_width=True)

        # Methodological note
        st.info("""
        **üìä Methodological Recommendation**

        Future regulatory impact studies should **always separate by document type**:

        1. **Privacy Policy**: Direct target of GDPR/CCPA ‚Üí Primary compliance indicator
        2. **Terms of Service**: Less directly regulated ‚Üí May show business strategy
        3. **Guidelines**: Community standards ‚Üí Different regulatory context

        Combining document types can produce **misleading conclusions** about compliance behavior.
        """)

    except FileNotFoundError:
        st.warning("Document type analysis not available. Run analyze_by_doc_type.py first.")

# TAB 9: CAUSAL ANALYSIS (DiD)
with tab9:
    st.subheader("Causal Analysis: Difference-in-Differences (DiD)")

    st.markdown("""
    **Research Question**: Did GDPR/CCPA cause platforms to change their policy language?

    We use Difference-in-Differences (DiD) to estimate the **causal effect** of regulations,
    not just correlation.
    """)

    # Load DiD results
    did_path = data_dir / 'did_analysis_results.json'

    try:
        with open(did_path) as f:
            did_results = json.load(f)

        # Show treatment/control groups
        config = did_results.get('config', {})

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**üéØ Treatment Group** (High EU Exposure)")
            treated = config.get('treated', {})
            st.write(", ".join(treated.get('platforms', [])))
            st.caption(treated.get('rationale', ''))

        with col2:
            st.markdown("**üîÑ Control Group** (Low EU Exposure)")
            control = config.get('control', {})
            st.write(", ".join(control.get('platforms', [])))
            st.caption(control.get('rationale', ''))

        st.markdown("---")

        # Results for each intervention
        for intervention_name, intervention_data in did_results.get('interventions', {}).items():
            st.subheader(f"üìÖ {intervention_name} Analysis")

            # Observation counts
            counts = intervention_data.get('observation_counts', {})
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Treated Pre", counts.get('treated_pre', 0))
            col2.metric("Treated Post", counts.get('treated_post', 0))
            col3.metric("Control Pre", counts.get('control_pre', 0))
            col4.metric("Control Post", counts.get('control_post', 0))

            # Main DiD result
            main_reg = intervention_data.get('main_regression', {})
            if 'error' not in main_reg:
                did_coef = main_reg.get('coefficients', {}).get('did_effect', {})

                col1, col2, col3 = st.columns(3)

                with col1:
                    estimate = did_coef.get('estimate', 0)
                    sig = "‚úÖ" if did_coef.get('significant_05') else ("‚ö†Ô∏è" if did_coef.get('significant_10') else "‚ùå")
                    st.metric(
                        "DiD Effect (Œ≤‚ÇÉ)",
                        f"{estimate:.4f}",
                        delta=f"p={did_coef.get('p_value', 1):.4f} {sig}"
                    )

                with col2:
                    ci = did_coef.get('ci_95', [0, 0])
                    st.metric(
                        "95% Confidence Interval",
                        f"[{ci[0]:.3f}, {ci[1]:.3f}]"
                    )

                with col3:
                    r2 = main_reg.get('r_squared', 0)
                    st.metric("R¬≤", f"{r2:.4f}")

                # Interpretation
                if did_coef.get('significant_05'):
                    st.success(f"**Statistically significant effect detected.** {intervention_name} caused treated platforms to change policy language by {estimate:.3f} index points more than control platforms.")
                elif did_coef.get('significant_10'):
                    st.warning(f"**Marginally significant effect.** Some evidence that {intervention_name} affected treated platforms (p < 0.10).")
                else:
                    st.info(f"**No significant causal effect detected.** The difference between treated and control groups after {intervention_name} is not statistically distinguishable from zero.")

            # Parallel Trends
            parallel = intervention_data.get('parallel_trends', {})
            if 'error' not in parallel:
                with st.expander("üìä Parallel Trends Assumption"):
                    assessment = parallel.get('assessment', 'Unknown')
                    if assessment == 'PASS':
                        st.success("‚úÖ Parallel trends assumption SUPPORTED")
                    else:
                        st.warning("‚ö†Ô∏è Parallel trends assumption may be VIOLATED - interpret with caution")

                    mean_test = parallel.get('tests', {}).get('mean_comparison', {})
                    st.write(f"Pre-treatment means: Treated = {mean_test.get('treated_mean', 0):.4f}, Control = {mean_test.get('control_mean', 0):.4f}")
                    st.write(f"Difference p-value: {mean_test.get('p_value', 1):.4f}")

            # Robustness
            robustness = intervention_data.get('robustness', {})
            if robustness:
                with st.expander("üîß Robustness Checks"):
                    if 'alternative_control' in robustness:
                        alt = robustness['alternative_control']
                        alt_reg = alt.get('regression', {})
                        if 'error' not in alt_reg:
                            alt_coef = alt_reg.get('coefficients', {}).get('did_effect', {})
                            st.write(f"**Alternative Control** ({', '.join(alt.get('control_group', []))})")
                            st.write(f"DiD Effect: {alt_coef.get('estimate', 0):.4f} (p={alt_coef.get('p_value', 1):.4f})")

                    if 'placebo_test' in robustness:
                        placebo = robustness['placebo_test']
                        placebo_reg = placebo.get('regression', {})
                        if 'error' not in placebo_reg:
                            placebo_coef = placebo_reg.get('coefficients', {}).get('did_effect', {})
                            placebo_pass = not placebo_coef.get('significant_10', False)
                            st.write(f"**Placebo Test** (1 year before)")
                            st.write(f"DiD Effect: {placebo_coef.get('estimate', 0):.4f} (p={placebo_coef.get('p_value', 1):.4f})")
                            if placebo_pass:
                                st.success("‚úÖ Placebo test PASSED - no spurious pre-trend")
                            else:
                                st.error("‚ùå Placebo test FAILED - possible confounding")

            st.markdown("---")

        # Methodology explanation
        with st.expander("üìö DiD Methodology Explained"):
            st.markdown("""
            **Difference-in-Differences (DiD)** is a causal inference method that compares:

            1. **Before vs After** (time effect)
            2. **Treated vs Control** (group effect)
            3. **The DIFFERENCE of these differences** (causal effect)

            **Model:**
            ```
            Y = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Post) + Œ≤‚ÇÇ(Treated) + Œ≤‚ÇÉ(Post √ó Treated) + Œµ
            ```

            - **Œ≤‚ÇÉ (DiD Effect)**: The causal effect of treatment
            - If Œ≤‚ÇÉ is significant, the regulation CAUSED the change

            **Key Assumption: Parallel Trends**
            - Treated and control groups would have followed similar trends WITHOUT treatment
            - We test this using pre-treatment data

            **Our Setup:**
            - **Treatment**: GDPR (2018) / CCPA (2020)
            - **Treated Group**: High EU exposure (Meta, YouTube, WhatsApp)
            - **Control Group**: Low EU exposure (Reddit, Twitter)
            """)

    except FileNotFoundError:
        st.warning("DiD analysis results not found. Please run `did_analysis_v2.py` first.")

        if st.button("Run DiD Analysis Now"):
            import subprocess
            result = subprocess.run(
                ['python3', str(data_dir.parent / 'scripts' / 'analysis' / 'did_analysis_v2.py')],
                capture_output=True,
                text=True
            )
            st.code(result.stdout)
            if result.returncode == 0:
                st.success("Analysis complete! Please refresh the page.")
            else:
                st.error(f"Error: {result.stderr}")

# TAB 10: VALIDATION & ROBUSTNESS
with tab10:
    st.subheader("Validation & Robustness Checks")

    st.markdown("""
    **Comprehensive validation** of the PAI methodology, addressing reviewer feedback
    on dimensional analysis, normalization, post-hoc rationalization, and null hypothesis testing.
    """)

    experiments_dir = data_dir / 'experiments'

    # --- Section 1: Permutation Test ---
    st.markdown("---")
    st.subheader("üé≤ Permutation Test: Lexicon vs Random N-grams")

    perm_path = experiments_dir / 'permutation_results.json'
    try:
        with open(perm_path) as f:
            perm_data = json.load(f)

        st.success(f"""
        ‚úÖ **{perm_data['metadata']['n_iterations']} permutations completed**
        ({perm_data['metadata']['n_documents']} documents, {perm_data['metadata'].get('runtime_seconds', '?')}s runtime)
        """)

        p_vals = perm_data['p_values']
        col1, col2, col3 = st.columns(3)

        with col1:
            p_var = p_vals['cross_platform_variance']
            sig = "‚úÖ" if p_var < 0.05 else "‚ùå"
            st.metric("Cross-Platform Variance", f"p = {p_var:.4f}", delta=f"{sig} {'Significant' if p_var < 0.05 else 'Not significant'}")

        with col2:
            p_rank = p_vals['ranking_stability']
            st.metric("Ranking Stability (œÑ‚â•0.8)", f"p = {p_rank:.4f}", delta=f"Mean œÑ = {perm_data['null_summary']['kendall_tau']['mean']:.3f}")

        with col3:
            p_gdpr = p_vals['gdpr_effect']
            sig = "‚úÖ" if p_gdpr < 0.05 else "‚ùå"
            st.metric("GDPR Effect", f"p = {p_gdpr:.4f}", delta=f"{sig} {'Significant' if p_gdpr < 0.05 else 'Not significant'}")

        with st.expander("üìä Detailed Permutation Results"):
            st.markdown(f"**Interpretation:** {perm_data.get('interpretation', 'N/A')}")

            real = perm_data['real_statistics']
            null = perm_data['null_summary']

            comp_data = []
            comp_data.append({
                'Statistic': 'Cross-Platform Variance',
                'Real': f"{real['cross_platform_variance']:.6f}",
                'Null Mean': f"{null['variance']['mean']:.6f}",
                'Null SD': f"{null['variance']['std']:.6f}",
                'p-value': f"{p_vals['cross_platform_variance']:.4f}"
            })
            comp_data.append({
                'Statistic': 'GDPR DiD Effect',
                'Real': f"{real['gdpr_did_effect']:.6f}",
                'Null Mean': f"{null['gdpr_did']['mean']:.6f}",
                'Null SD': f"{null['gdpr_did']['std']:.6f}",
                'p-value': f"{p_vals['gdpr_effect']:.4f}"
            })
            st.table(comp_data)

            st.markdown("**Platform Rankings (Real Lexicon):**")
            rank_data = [{'Platform': p, 'Rank': r} for p, r in real['platform_rankings'].items()]
            rank_data.sort(key=lambda x: x['Rank'])
            st.table(rank_data)

    except FileNotFoundError:
        st.warning("‚è≥ Permutation test results not yet available. Run `permutation_test.py` first.")

    # --- Section 2: Normalization Sensitivity ---
    st.markdown("---")
    st.subheader("üìê Normalization Sensitivity Analysis")

    norm_path = experiments_dir / 'sensitivity_results.json'
    try:
        with open(norm_path) as f:
            norm_data = json.load(f)

        summary = norm_data['summary']
        col1, col2 = st.columns(2)

        with col1:
            st.metric("Mean Spearman œÅ", f"{summary['mean_spearman_rho']:.3f}")
        with col2:
            st.metric("Assessment", summary['stability_assessment'].split(':')[0])

        # Correlation heatmap
        corr = norm_data['pairwise_correlations']
        methods = ['per_1k', 'per_sentence', 'tfidf', 'raw']
        labels = ['Per-1k-word', 'Per-sentence', 'TF-IDF', 'Raw counts']

        corr_matrix = []
        for i, m1 in enumerate(methods):
            row = []
            for j, m2 in enumerate(methods):
                if i == j:
                    row.append(1.0)
                else:
                    key = f"{m1}_vs_{m2}" if f"{m1}_vs_{m2}" in corr else f"{m2}_vs_{m1}"
                    row.append(corr.get(key, {}).get('rho', 0))
            corr_matrix.append(row)

        fig_corr = go.Figure(data=go.Heatmap(
            z=corr_matrix,
            x=labels,
            y=labels,
            text=[[f"{v:.3f}" for v in row] for row in corr_matrix],
            texttemplate="%{text}",
            colorscale='RdYlGn',
            zmin=0.5, zmax=1.0
        ))
        fig_corr.update_layout(title="Spearman Rank Correlation Between Normalization Methods", height=400)
        st.plotly_chart(fig_corr, use_container_width=True)

        with st.expander("üìä Rankings by Method"):
            results = norm_data['results_by_method']
            rank_compare = []
            for method_key, method_data in results.items():
                for i, p in enumerate(method_data['ranking']):
                    rank_compare.append({
                        'Method': method_data['name'],
                        'Platform': p,
                        'Rank': i + 1
                    })
            df_ranks = pd.DataFrame(rank_compare)
            pivot = df_ranks.pivot(index='Platform', columns='Method', values='Rank')
            st.dataframe(pivot, use_container_width=True)

    except FileNotFoundError:
        st.warning("Normalization sensitivity results not available.")

    # --- Section 3: Bootstrap Rank CI ---
    st.markdown("---")
    st.subheader("üìä Bootstrap Rank Confidence Intervals")

    boot_path = experiments_dir / 'bootstrap_rank_results.json'
    try:
        with open(boot_path) as f:
            boot_data = json.load(f)

        st.info(f"**{boot_data['metadata']['n_iterations']:,} bootstrap iterations** | {boot_data['metadata']['n_documents']} documents | {boot_data['metadata']['n_platforms']} platforms")

        # Rank CI chart
        ci_data = boot_data['rank_confidence_intervals']
        platforms_sorted = sorted(ci_data.keys(), key=lambda p: ci_data[p]['mean_rank'])

        fig_ci = go.Figure()
        for p in platforms_sorted:
            d = ci_data[p]
            color = PLATFORM_CONFIG.get(p, {}).get('color', '#888888')
            fig_ci.add_trace(go.Bar(
                x=[p],
                y=[d['mean_rank']],
                error_y=dict(
                    type='data',
                    symmetric=False,
                    array=[d['ci_upper'] - d['mean_rank']],
                    arrayminus=[d['mean_rank'] - d['ci_lower']]
                ),
                marker_color=color,
                name=p,
                showlegend=False,
                text=[f"{d['mean_rank']:.1f} [{d['ci_lower']}-{d['ci_upper']}]"],
                textposition='outside'
            ))

        fig_ci.update_layout(
            title="Platform Rank with 95% Bootstrap CI (lower = more platform-protective)",
            yaxis_title="Rank (1 = highest PAI)",
            yaxis=dict(autorange='reversed'),
            height=450
        )
        st.plotly_chart(fig_ci, use_container_width=True)

        # Rank reversal heatmap
        with st.expander("üîÑ Rank Reversal Probability Matrix"):
            reversal = boot_data['rank_reversal_probability']
            plats = sorted(reversal.keys())
            rev_matrix = [[reversal[p1].get(p2, 0) for p2 in plats] for p1 in plats]

            fig_rev = go.Figure(data=go.Heatmap(
                z=rev_matrix,
                x=plats, y=plats,
                text=[[f"{v:.2f}" for v in row] for row in rev_matrix],
                texttemplate="%{text}",
                colorscale='RdBu',
                zmin=0, zmax=1
            ))
            fig_rev.update_layout(title="P(row platform ranked higher than column platform)", height=500)
            st.plotly_chart(fig_rev, use_container_width=True)

            st.caption("Values > 0.5 mean the row platform is typically ranked higher (more platform-protective). Values near 0.5 indicate uncertain ordering.")

        if boot_data.get('insufficient_data_platforms'):
            st.warning(f"‚ö†Ô∏è Insufficient data: {', '.join(boot_data['insufficient_data_platforms'])}")

    except FileNotFoundError:
        st.warning("Bootstrap results not available.")

    # --- Section 4: Segmented Regression ---
    st.markdown("---")
    st.subheader("üìà Segmented Regression (ITS v2)")

    seg_path = experiments_dir / 'segmented_regression_results_v2.json'
    try:
        with open(seg_path) as f:
            seg_data = json.load(f)

        for platform_name in ['Meta', 'Reddit']:
            if platform_name not in seg_data.get('platforms', {}):
                continue
            pdata = seg_data['platforms'][platform_name]

            with st.expander(f"üìà {platform_name} (n={pdata['n']})"):
                if 'regression' in pdata:
                    reg = pdata['regression']
                    coefs = reg['coefficients']
                    pvals = reg['p_values']

                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("R¬≤", f"{reg['r_squared']:.3f}")
                    col2.metric("Level Change (Œ≤‚ÇÇ)", f"{coefs['level_change']:+.3f}", delta=f"p={pvals['level_change']:.3f}")
                    col3.metric("Slope Change (Œ≤‚ÇÉ)", f"{coefs['slope_change']:+.4f}", delta=f"p={pvals['slope_change']:.3f}")
                    col4.metric("Cohen's d", f"{pdata.get('cohens_d', 'N/A')}")

                    coef_table = []
                    for key in ['intercept', 'time_trend', 'level_change', 'slope_change']:
                        coef_table.append({
                            'Coefficient': key,
                            'Estimate': f"{coefs[key]:+.4f}",
                            'HC3 SE': f"{reg['se_hc3'][key]:.4f}",
                            't-stat': f"{reg['t_statistics'][key]:.4f}",
                            'p-value': f"{pvals[key]:.4f}"
                        })
                    st.table(coef_table)
                else:
                    st.write(f"Pre/Post comparison only: Cohen's d = {pdata.get('cohens_d', 'N/A')}")

        # Effect size summary for all platforms
        st.markdown("**Effect Size Summary (All Platforms)**")
        effect_data = []
        for pname, pdata in seg_data.get('platforms', {}).items():
            effect_data.append({
                'Platform': pname,
                'n': pdata['n'],
                'Mean Pre': f"{pdata.get('mean_pre', 'N/A'):.4f}" if pdata.get('mean_pre') is not None else 'N/A',
                'Mean Post': f"{pdata.get('mean_post', 'N/A'):.4f}" if pdata.get('mean_post') is not None else 'N/A',
                "Cohen's d": f"{pdata['cohens_d']:.3f}" if pdata.get('cohens_d') is not None else 'N/A',
                'Method': 'Seg. Regression' if 'regression' in pdata else 'Pre/Post only'
            })
        st.table(effect_data)

    except FileNotFoundError:
        st.warning("Segmented regression results not available.")

    # --- Section 5: DiD v3 ---
    st.markdown("---")
    st.subheader("‚öñÔ∏è Difference-in-Differences v3 (Lexicon-Based)")

    did3_path = experiments_dir / 'did_results_v3.json'
    try:
        with open(did3_path) as f:
            did3_data = json.load(f)

        st.info(f"""
        **Primary outcome**: combined_index (lexicon-based) | **SE type**: {did3_data['metadata']['se_type']} |
        **n**: {did3_data['metadata']['n_total']}
        """)

        sensitivity = did3_data['treatment_sensitivity']

        did_summary = []
        for threshold, tdata in sensitivity.items():
            ci_did = tdata['combined_index_did']
            leg_did = tdata.get('legalization_index_did', {})
            did_coef = ci_did['coefficients']['did_effect']
            did_p = ci_did['p_values']['did_effect']

            did_summary.append({
                'Threshold': threshold,
                'Treated': ', '.join(tdata['treated_platforms']),
                'n_treated': tdata['n_treated'],
                'DiD (combined)': f"{did_coef:+.3f}",
                'p-value': f"{did_p:.3f}" if not pd.isna(did_p) else 'NaN',
                'Sig?': '‚úÖ' if did_p < 0.05 else '‚ùå',
                'DiD (legalization)': f"{leg_did.get('did_effect', 0):+.3f}",
                'MDE': f"{tdata['power_analysis']['mde_80pct_power']:.3f}"
            })

        st.table(did_summary)

        st.warning("""
        **‚ö†Ô∏è Interpretation**: DiD effects are **not statistically significant** across all treatment thresholds.
        However, effects are consistently **positive** (treated platforms show higher PAI increase),
        and the MDE = 0.382 SD suggests we may lack power to detect true effects of this magnitude.
        This is an exploratory finding ‚Äî "absence of evidence is not evidence of absence."
        """)

    except FileNotFoundError:
        st.warning("DiD v3 results not available.")

    # --- Overall Assessment ---
    st.markdown("---")
    st.subheader("üìã Overall Validation Summary")

    st.markdown("""
    | Check | Status | Key Result |
    |-------|--------|------------|
    | **Normalization Sensitivity** | ‚úÖ Stable | Mean œÅ = 0.80, top/bottom rankings consistent |
    | **Bootstrap Rank CI** | ‚úÖ Robust | Top-3 and bottom-3 stable across 10K iterations |
    | **Segmented Regression** | ‚ö†Ô∏è Underpowered | Good fit (R¬≤>0.85) but n=12 limits significance |
    | **DiD v3** | ‚ö†Ô∏è Underpowered | Consistent direction, MDE = 0.382 SD |
    | **Placebo Tests** | ‚ö†Ô∏è Limited | Sparse data prevents meaningful placebo variation |
    | **Permutation Test** | üîÑ See above | 1,000 iterations with random n-gram lexicons |
    """)

# Sidebar: Methodology
st.sidebar.markdown("---")
st.sidebar.header("üìö Methodology v3")

with st.sidebar.expander("üìä Index Calculation"):
    if pca_index:
        st.markdown(f"""
        **PCA-Weighted Composite Index**
        
        PC1 Explained Variance: **{pca_index['methodology']['explained_variance']:.1%}**
        
        **Dimensions:**
        1. Complexity (ADL, tree depth)
        2. Agency (visibility asymmetry)
        3. Formality (legal terminology)
        4. Discretion (corpus-derived) *NEW*
        
        **Interpretation:**
        Higher score = more legalized = more platform-protective
        """)
    else:
        st.markdown("PCA Index data not loaded.")

with st.sidebar.expander("üìú Discretion Lexicon"):
    st.markdown("""
    **Corpus-Derived Approach**
    
    Unlike generic lexicons, our discretion terms were
    extracted directly from the 50 Tier 1 policy documents.
    
    **Key Patterns Found:**
    - "at any time" (113x)
    - "reserve the right" (90x)
    - "sole discretion" (56x)
    - "without limitation" (32x)
    
    This ensures no platform-specific language is missed.
    """)

with st.sidebar.expander("üìà ITS Analysis"):
    st.markdown("""
    **Interrupted Time Series**
    
    Tests whether regulatory events (GDPR, CCPA) caused
    statistically significant changes in policy trends.
    
    **Method:**
    - Segment time series at event date
    - Compare pre/post trend slopes
    - Test for significance (p < 0.05)
    - Measure immediate level changes
    """)

with st.sidebar.expander("Confidence Levels"):
    st.markdown("""
    - **High (‚úì):** ‚â•8 documents, validated metrics
    - **Medium (‚ö†):** 5-7 documents
    - **Low (‚ö†Ô∏è):** <5 documents, preliminary
    
    **TikTok** has low confidence (only 4 docs) but shows 
    clear outlier pattern.
    """)

# Footer
st.markdown("---")
st.caption("""
**Data Version:** 2026-01-22 (Audit Verified) | **Platforms:** 7 (All Real Temporal Data) |
**Model:** SpaCy Legal NLP | **Corpus:** Tier 1 Social Media (1,273 documents)
""")
